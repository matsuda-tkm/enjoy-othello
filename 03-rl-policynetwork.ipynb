{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 方針\n- このノートブックでは、学習済みSLポリシーネットワークの重みを初期値としてREINFORCEにてRLポリシーネットワークを学習させる。\n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install creversi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-01T15:18:54.505162Z","iopub.execute_input":"2023-09-01T15:18:54.505575Z","iopub.status.idle":"2023-09-01T15:19:06.071664Z","shell.execute_reply.started":"2023-09-01T15:18:54.505538Z","shell.execute_reply":"2023-09-01T15:19:06.070129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# リバーシ用ライブラリ\nfrom creversi import Board,move_to_str,move_from_str,move_rotate90,move_rotate180,move_rotate270\nimport creversi\n# 基礎ライブラリ\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom copy import copy,deepcopy\nimport gc\nimport os\n# 学習用ライブラリ\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:19:06.073925Z","iopub.execute_input":"2023-09-01T15:19:06.074831Z","iopub.status.idle":"2023-09-01T15:19:10.002077Z","shell.execute_reply.started":"2023-09-01T15:19:06.074798Z","shell.execute_reply":"2023-09-01T15:19:10.000250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PolicyNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        n_filters = 80\n        self.input_layer = nn.Sequential(\n            nn.Conv2d(8,n_filters,kernel_size=5,padding=2),\n            nn.ReLU()\n        )\n        self.hidden_layer = nn.Sequential(\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.ReLU()\n        )\n        self.output_layer = nn.Sequential(\n            nn.Conv2d(n_filters,1,kernel_size=1),\n            nn.Flatten()\n        )\n        \n    def forward(self,x):\n        out = self.input_layer(x)\n        out = self.hidden_layer(out)\n        out = self.output_layer(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:19:10.003743Z","iopub.execute_input":"2023-09-01T15:19:10.004460Z","iopub.status.idle":"2023-09-01T15:19:10.015149Z","shell.execute_reply.started":"2023-09-01T15:19:10.004429Z","shell.execute_reply":"2023-09-01T15:19:10.013592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PolicyNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        n_filters = 100\n        self.input_layer = nn.Sequential(\n            nn.Conv2d(8,n_filters,kernel_size=5,padding=2),\n            nn.ReLU()\n        )\n        self.hidden_layer = nn.Sequential(\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU(),\n            nn.Conv2d(n_filters,n_filters,kernel_size=3,padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU()\n        )\n        self.output_layer = nn.Sequential(\n            nn.Conv2d(n_filters,1,kernel_size=1),\n            nn.Flatten()\n        )\n        \n    def forward(self,x):\n        out = self.input_layer(x)\n        out = self.hidden_layer(out)\n        out = self.output_layer(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:19:34.485911Z","iopub.execute_input":"2023-09-01T15:19:34.486429Z","iopub.status.idle":"2023-09-01T15:19:34.500048Z","shell.execute_reply.started":"2023-09-01T15:19:34.486388Z","shell.execute_reply":"2023-09-01T15:19:34.498868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def board_to_array(board):\n    \"\"\"\n    boardオブジェクトからndarrayに変換する関数。\n    第1チャンネルは黒石の位置、第2チャンネルに白石の位置、第3チャンネルに空白の位置、\n    第4チャンネルに合法手の位置、第5チャンネルに返せる石の個数、第6チャンネルに隅=1、\n    第7チャンネルに1埋め、第8チャンネルに0埋め。\n    \"\"\"\n    b = np.zeros((8,8,8), dtype=np.float32)\n    board.piece_planes(b)\n    if not board.turn:\n        b = b[[1,0,2,3,4,5,6,7],:,:]\n    b[2] = np.where(b[0]+b[1]==1, 0, 1)\n    legal_moves = list(board.legal_moves)\n    if legal_moves != [64]:\n        n_returns = []\n        for move in legal_moves:\n            board_ = copy(board)\n            n_before = board_.opponent_piece_num()\n            board_.move(move)\n            n_after = board_.piece_num()\n            n_returns.append(n_before-n_after)\n        tmp = np.zeros(64)\n        tmp[legal_moves] = n_returns\n        tmp = tmp.reshape(8,8)\n        b[3] = np.where(tmp > 0,1,0)\n        b[4] = tmp\n    b[5] = np.array([1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., \n                     0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., \n                     0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                     1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.]).reshape(8,8)\n    b[6] = 1\n    return b","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:19:35.367716Z","iopub.execute_input":"2023-09-01T15:19:35.368101Z","iopub.status.idle":"2023-09-01T15:19:35.378120Z","shell.execute_reply.started":"2023-09-01T15:19:35.368070Z","shell.execute_reply":"2023-09-01T15:19:35.376793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(board, model):\n    \"\"\"モデルの出力のうち、合法手のみを選びsoftmaxに通す。合法手のidxも返す。\"\"\"\n    model.eval()\n    device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n#     board_arr = torch.from_numpy(board_to_array(board)).to(device)\n    board_arr = torch.from_numpy(board_to_array(board)).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(board_arr)[0]\n    legal_moves = list(board.legal_moves)\n    return output[legal_moves].softmax(dim=0).numpy(), legal_moves","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:19:58.213291Z","iopub.execute_input":"2023-09-01T15:19:58.213654Z","iopub.status.idle":"2023-09-01T15:19:58.219078Z","shell.execute_reply.started":"2023-09-01T15:19:58.213629Z","shell.execute_reply":"2023-09-01T15:19:58.218190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_play(model, N, greedy=False):\n    Z = []\n    for i in range(N):\n        board = Board()\n        while not board.is_game_over():\n            ## 自分の手番\n            if board.turn:\n                if 64 not in list(board.legal_moves):\n                    ### 推論\n                    model.eval()\n                    model.to(device)\n                    # board_arr = torch.from_numpy(board_to_array(board)).to(device)\n                    board_arr = torch.from_numpy(board_to_array(board)).unsqueeze(0).to(device)\n                    output = model(board_arr)[0].cpu()\n                    legal_moves = list(board.legal_moves)\n                    prob = output[legal_moves].softmax(dim=0).detach().numpy()\n                    ### 行動\n                    if greedy:\n                        move = legal_moves[prob.argmax()]\n                    else:\n                        move = np.random.choice(legal_moves, p=prob)\n                    board.move(move)\n                else:\n                    board.move_pass()\n            ## 相手の手番\n            else:\n                legal_moves = list(board.legal_moves)\n                board.move(np.random.choice(legal_moves))\n\n        # 勝ち負けをzに格納\n        if board.turn:\n            z = board.diff_num()\n        else:\n            z = -board.diff_num()\n        Z.append(z)\n    Z = np.array(Z)\n    return (Z>0).sum()/N, Z.mean(), Z.std(), Z.min(), Z.max()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:20:34.175306Z","iopub.execute_input":"2023-09-01T15:20:34.175681Z","iopub.status.idle":"2023-09-01T15:20:34.185028Z","shell.execute_reply.started":"2023-09-01T15:20:34.175647Z","shell.execute_reply":"2023-09-01T15:20:34.183638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 学習\n\n$$\n\\nabla_\\theta J(\\theta)=E\\left[\\sum_{t=0}^T G_t\\nabla_\\theta\\log\\pi_\\theta(A_t|S_t)\\right]\\\\\nG_t \\equiv R_t + \\gamma R_{t+1} + \\cdots + \\gamma^{T-t}R_T\n$$\n\nつまり、損失関数は\n\n$$\n\\mathcal{L}=E\\left[\\sum_{t=0}^T G_t\\log\\pi_\\theta(A_t|S_t)\\right]\n$$\n","metadata":{}},{"cell_type":"code","source":"# for file in sorted(os.listdir('/kaggle/input/reversi-datasets/results')):\n#     if '.pth' in file:\n#         # モデル読み込み\n#         print(file)\n#         model = torch.load(f'/kaggle/input/reversi-datasets/results/{file}')\n#         device = 'cuda' if torch.cuda.is_available() else 'cpu'\n#         model = model.to(device)\n#         ratio,mean,std,m,M = test_play(model, 1000)\n#         print(f'ratio:{ratio*100:.1f}%, mean:{mean:.1f}, std:{std:.1f}, min:{m:.0f}, max:{M:.0f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-30T06:33:51.239815Z","iopub.execute_input":"2023-08-30T06:33:51.240213Z","iopub.status.idle":"2023-08-30T06:33:51.252120Z","shell.execute_reply.started":"2023-08-30T06:33:51.240182Z","shell.execute_reply":"2023-08-30T06:33:51.251067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# モデル読み込み\nmodel = torch.load('/kaggle/input/reversi-datasets/SL-PolicyNetwork-v3-checkpoint-5epoch-subdata99.pth')\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:20:37.406502Z","iopub.execute_input":"2023-09-01T15:20:37.406921Z","iopub.status.idle":"2023-09-01T15:20:37.580700Z","shell.execute_reply.started":"2023-09-01T15:20:37.406889Z","shell.execute_reply":"2023-09-01T15:20:37.578504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratio,mean,std,m,M = test_play(model, 1000, greedy=True)\nprint(f'ratio:{ratio*100:.1f}%, mean:{mean:.1f}, std:{std:.1f}, min:{m:.0f}, max:{M:.0f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:20:42.921378Z","iopub.execute_input":"2023-09-01T15:20:42.921783Z","iopub.status.idle":"2023-09-01T15:22:53.010385Z","shell.execute_reply.started":"2023-09-01T15:20:42.921752Z","shell.execute_reply":"2023-09-01T15:22:53.009597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- prob.....ratio:91.2%, mean:23.6, std:16.1, min:-36, max:60  \n- greedy...ratio:97.7%, mean:35.5, std:14.3, min:-34, max:64\n- new......ratio:99.3%, mean:40.1, std:12.6, min:-28, max:64","metadata":{}},{"cell_type":"code","source":"# 設定\nn_episode = 100  # 1セット当たりの対局数、相手モデルの更新頻度\nn_set = 1  # セット数\nlearning_rate = 0.0001\ngamma = 0.99\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:23:15.851596Z","iopub.execute_input":"2023-09-01T15:23:15.852075Z","iopub.status.idle":"2023-09-01T15:23:15.859791Z","shell.execute_reply.started":"2023-09-01T15:23:15.852039Z","shell.execute_reply":"2023-09-01T15:23:15.857948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_op_list = []  # 過去の相手モデル集合\n\nfor i in range(n_set):\n    # 相手モデルの更新\n    model_op_list.append(deepcopy(model.to(device)).eval())\n    model_op = deepcopy(np.random.choice(model_op_list))\n\n    for n in tqdm(range(n_episode)):\n        # (prob,r)を格納する配列\n        tau = [[],[]]  # 自分側\n        tau_op = [[],[]]  # 相手側\n\n        # 自己対局\n        board = Board()\n        while not board.is_game_over():\n            ## 自分の手番\n            if board.turn:\n                if 64 not in list(board.legal_moves):\n                    ### 推論\n                    model.eval()\n                    # board_arr = torch.from_numpy(board_to_array(board)).to(device)\n                    board_arr = torch.from_numpy(board_to_array(board)).unsqueeze(0).to(device)\n                    output = model(board_arr)[0].cpu()\n                    legal_moves = list(board.legal_moves)\n                    prob = output[legal_moves].softmax(dim=0).detach().numpy()\n                    ### 行動\n                    move = np.random.choice(legal_moves, p=prob)\n                    tau[0].append(output.softmax(0)[move])\n                    tau[1].append(0)\n                    board.move(move)\n                else:\n                    board.move_pass()\n            ## 相手の手番\n            else:\n                if 64 not in list(board.legal_moves):\n                    ### 推論\n                    # board_arr = torch.from_numpy(board_to_array(board)).to(device)\n                    board_arr = torch.from_numpy(board_to_array(board)).unsqueeze(0).to(device)\n                    output = model_op(board_arr)[0].cpu()\n                    legal_moves = list(board.legal_moves)\n                    prob = output[legal_moves].softmax(dim=0).detach().numpy()\n                    ### 行動\n                    move = np.random.choice(legal_moves, p=prob)\n                    tau_op[0].append(output.softmax(0)[move])\n                    tau_op[1].append(0)\n                    board.move(move)\n                else:\n                    board.move_pass()\n\n        # 勝ち負けをzに格納\n        if board.diff_num() == 0:\n            z = 0\n        if board.turn:\n            if board.diff_num() > 0:\n                z = 1\n            else:\n                z = -1\n        else:\n            if board.diff_num() < 0:\n                z = 1\n            else:\n                z = -1\n\n        tau[1][-1] = z\n        tau_op[1][-1] = -z\n        \n        # 損失を計算1\n        model.train()\n        optimizer.zero_grad()\n        loss = 0.\n        reward = 0.\n        for p,r in zip(reversed(tau[0]),reversed(tau[1])):\n            reward = gamma*reward + r\n            loss += -torch.log(p) * reward\n        loss.backward()\n        optimizer.step()\n        \n        # 損失を計算2\n        optimizer.zero_grad()\n        loss = 0.\n        reward = 0.\n        for p,r in zip(reversed(tau_op[0]),reversed(tau_op[1])):\n            reward = gamma*reward + r\n            loss += -torch.log(p) * reward\n        loss.backward()\n        optimizer.step()\n\n    torch.save(model.cpu(), f'RL-PolicyNetwork-checkpoint-{i+1}.pth')\n    ratio,mean,std,m,M = test_play(model, 1000, greedy=True)\n    print(f'[{i+1}/{n_set}] ratio:{ratio*100:.1f}%, mean:{mean:.1f}, std:{std:.1f}, min:{m:.0f}, max:{M:.0f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:24:23.062754Z","iopub.execute_input":"2023-09-01T15:24:23.063107Z","iopub.status.idle":"2023-09-01T15:32:27.017824Z","shell.execute_reply.started":"2023-09-01T15:24:23.063081Z","shell.execute_reply":"2023-09-01T15:32:27.016564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = torch.load('/kaggle/working/RL-PolicyNetwork-checkpoint-1.pth')","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:33:19.299555Z","iopub.execute_input":"2023-09-01T15:33:19.299930Z","iopub.status.idle":"2023-09-01T15:33:19.313837Z","shell.execute_reply.started":"2023-09-01T15:33:19.299900Z","shell.execute_reply":"2023-09-01T15:33:19.312899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"board = Board()\nwhile not board.is_game_over():\n    ## 自分の手番\n    if board.turn:\n        if 64 not in list(board.legal_moves):\n            ### 推論\n            model.eval()\n            model.to(device)\n            # board_arr = torch.from_numpy(board_to_array(board)).to(device)\n            board_arr = torch.from_numpy(board_to_array(board)).unsqueeze(0).to(device)\n            output = model(board_arr)[0].cpu()\n            legal_moves = list(board.legal_moves)\n            prob = output[legal_moves].softmax(dim=0).detach().numpy()\n            ### 可視化\n            display(board)\n            plt.figure(figsize=(3,3))\n            p = output.softmax(0).detach().numpy().reshape(8,8)*100\n            sns.heatmap(p, cmap='gray_r', fmt='.0f', annot=True, cbar=False)\n            plt.show()\n            ### 行動\n            move = legal_moves[prob.argmax()]\n            board.move(move)\n        else:\n            board.move_pass()\n    ## 相手の手番\n    else:\n        if 64 not in list(board.legal_moves):\n            ### 推論\n            model.eval()\n            model.to(device)\n            # board_arr = torch.from_numpy(board_to_array(board)).to(device)\n            board_arr = torch.from_numpy(board_to_array(board)).unsqueeze(0).to(device)\n            output = model_op_list[0](board_arr)[0].cpu()\n            legal_moves = list(board.legal_moves)\n            prob = output[legal_moves].softmax(dim=0).detach().numpy()\n            ### 行動\n            move = np.random.choice(legal_moves, p=prob)\n            board.move(move)\n        else:\n            board.move_pass()\ndisplay(board)\nboard.diff_num()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T15:34:24.307500Z","iopub.execute_input":"2023-09-01T15:34:24.307908Z","iopub.status.idle":"2023-09-01T15:34:32.405436Z","shell.execute_reply.started":"2023-09-01T15:34:24.307881Z","shell.execute_reply":"2023-09-01T15:34:32.403745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}